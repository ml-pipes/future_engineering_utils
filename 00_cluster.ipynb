{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fe_utils.cluster\n",
    "> module for clustering optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial\n",
    "import hdbscan\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, space_eval\n",
    "from hyperopt import Trials\n",
    "from hyperopt import STATUS_OK\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import homogeneity_score, v_measure_score, silhouette_score \n",
    "from sklearn.metrics.cluster import completeness_score, adjusted_mutual_info_score\n",
    "import time\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def load_embedding(path):\n",
    "    \"\"\"\n",
    "    dataloader for saved embeddings. Load from numpy file\n",
    "    \"\"\"\n",
    "    emb = np.load(path)\n",
    "    \n",
    "    return emb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BayesClusterTrainer():\n",
    "    \"\"\"\n",
    "    A trainer for cluster optimization runs\n",
    "    Inputs:\n",
    "        `space`: `dict` containing relevant parameter spaces for `hdbscan` and `umap`\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, space, cost_fn_params, embeddings, labels, optimize='default' , *args, **kwargs):\n",
    "        self.space = space\n",
    "        self.cost_fn_params = cost_fn_params\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.optimize = optimize\n",
    "\n",
    "        self.logs = []\n",
    "\n",
    "        self.run = dict()\n",
    "\n",
    "    def generate_clusters(self, embeddings,\n",
    "                        min_cluster_size,\n",
    "                        cluster_selection_epsilon,\n",
    "                        cluster_selection_method,\n",
    "                        metric,\n",
    "                        n_neighbors,\n",
    "                        n_components, \n",
    "                        random_state = 42):\n",
    "        \"\"\"\n",
    "        Generate HDBSCAN cluster object after reducing embedding dimensionality with UMAP\n",
    "        \"\"\"\n",
    "    \n",
    "        umap_embeddings = (umap.UMAP(n_neighbors=n_neighbors, \n",
    "                                    n_components=n_components, \n",
    "                                    metric='cosine', \n",
    "                                    random_state=random_state)\n",
    "                                    .fit_transform(embeddings))\n",
    "\n",
    "        clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,\n",
    "                                   metric=metric, cluster_selection_epsilon = cluster_selection_epsilon,\n",
    "                                   cluster_selection_method=cluster_selection_method,\n",
    "                                   gen_min_span_tree=True).fit(umap_embeddings)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def objective(self, params, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Objective function for hyperopt to minimize, which incorporates constraints\n",
    "        on the number of clusters we want to identify\n",
    "        \"\"\"\n",
    "    \n",
    "        clusters = self.generate_clusters(embeddings, \n",
    "                                     n_neighbors = params['n_neighbors'], \n",
    "                                     n_components = params['n_components'], \n",
    "                                     min_cluster_size = params['min_cluster_size'],\n",
    "                                     cluster_selection_epsilon = params['cluster_selection_epsilon'],\n",
    "                                     metric = params['metric'],\n",
    "                                     cluster_selection_method = params['cluster_selection_method'],\n",
    "                                     random_state = 42)\n",
    "\n",
    "        cost = self.score_clusters(clusters, labels, params)\n",
    "\n",
    "        loss = cost\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    def score_clusters(self, clusters, y, params):\n",
    "        \"\"\"\n",
    "        Returns the label count and cost of a given cluster supplied from running hdbscan\n",
    "        \"\"\"\n",
    "        \n",
    "        val = clusters.relative_validity_\n",
    "        \n",
    "        #prevent pers from getting NaN value if no clusters exist\n",
    "        if len(clusters.cluster_persistence_)==0:\n",
    "            pers = 0.\n",
    "        else:\n",
    "            pers = clusters.cluster_persistence_.mean(0)\n",
    "\n",
    "        prob = clusters.probabilities_.mean(0)          \n",
    "\n",
    "        penalty = (clusters.labels_ == -1).sum() / len(clusters.labels_)\n",
    "        \n",
    "        if len(clusters.outlier_scores_)==0:\n",
    "            outlier = 0.0\n",
    "        else:\n",
    "            outlier = clusters.outlier_scores_.mean(0)\n",
    "            \n",
    "\n",
    "        cluster_size = len(np.unique(clusters.labels_))\n",
    "        \n",
    "        \n",
    "        self.run['relative_validity'] = val\n",
    "        self.run['probability'] = prob\n",
    "        self.run['persistence'] = pers\n",
    "        self.run['penalty'] = penalty\n",
    "        self.run['outlier'] = outlier\n",
    "        self.run['cluster_size'] = cluster_size\n",
    "        \n",
    "        fns = [adjusted_rand_score, homogeneity_completeness_v_measure, homogeneity_score, v_measure_score, completeness_score, adjusted_mutual_info_score]\n",
    "\n",
    "        for fn in fns:\n",
    "            print(f\"{fn.__name__} : {fn(clusters.labels_, y)}\")      \n",
    "            self.run[f'{fn.__name__}'] = fn(clusters.labels_, y)\n",
    "        print(\"-\"*20)\n",
    "        \n",
    "        \n",
    "        if self.optimize == 'rand_score':\n",
    "\n",
    "            score = -1. * adjusted_rand_score(clusters.labels_, y)\n",
    "            self.run['score'] = score\n",
    "            \n",
    "            self.run = {**self.run, **params}\n",
    "\n",
    "            self.logs.append(self.run.copy())\n",
    "            self.run.clear()\n",
    "            \n",
    "            print(f\"SCORE: {score}\")\n",
    "\n",
    "            return score\n",
    "        \n",
    "        elif self.optimize == 'default':\n",
    "        \n",
    "            print(f'val: {val}')\n",
    "            print(f'pers: {pers}')\n",
    "            print(f'prob: {prob}')\n",
    "            print(f'penalty: {penalty}')\n",
    "            print(f'outlier: {outlier}')\n",
    "            print(f'cluster size: {cluster_size}')\n",
    "\n",
    "            val_w = self.cost_fn_params['val_w']\n",
    "            prob_w = self.cost_fn_params['prob_w']\n",
    "            pers_w = self.cost_fn_params['pers_w']\n",
    "            penalty_w = self.cost_fn_params['penalty_w']\n",
    "            outlier_w = self.cost_fn_params['outlier_w']\n",
    "\n",
    "            score = -1*(val_w * val + prob_w * prob + pers_w * pers) + (penalty_w * penalty +  outlier_w * outlier)       \n",
    "\n",
    "            self.run['score'] = score\n",
    "\n",
    "            self.run = {**self.run, **self.cost_fn_params, **params}\n",
    "\n",
    "            self.logs.append(self.run.copy())\n",
    "            self.run.clear()\n",
    "\n",
    "            print(f\"SCORE: {score}\")\n",
    "\n",
    "            return score\n",
    "\n",
    "\n",
    "    def train(self, max_evals=100, algo=tpe.suggest):\n",
    "        \"\"\"\n",
    "        Perform bayesian search on hyperopt hyperparameter space to minimize objective function\n",
    "        \"\"\"\n",
    "    \n",
    "        trials = Trials()\n",
    "        fmin_objective = partial(self.objective, embeddings=self.embeddings, labels=self.labels)\n",
    "        best = fmin(fmin_objective, \n",
    "                    space = self.space, \n",
    "                    algo=algo,\n",
    "                    max_evals=max_evals, \n",
    "                    trials=trials)\n",
    "\n",
    "        best_params = space_eval(self.space, best)\n",
    "        print ('best:')\n",
    "        print (best_params)\n",
    "        print(\"-\"*20)\n",
    "        print(\"-\"*20)\n",
    "\n",
    "        best_clusters = self.generate_clusters(self.embeddings, \n",
    "                                         n_neighbors = best_params['n_neighbors'], \n",
    "                                         n_components = best_params['n_components'], \n",
    "                                         min_cluster_size = best_params['min_cluster_size'],\n",
    "                                         cluster_selection_epsilon = best_params['cluster_selection_epsilon'],\n",
    "                                         metric = best_params['metric'],\n",
    "                                         cluster_selection_method = best_params['cluster_selection_method']\n",
    "                                         )\n",
    "        \n",
    "        \n",
    "        self.best_params = best_params\n",
    "        self.best_clusters = best_clusters\n",
    "        self.trials = trials\n",
    "        \n",
    "        print(f'Finished training!')\n",
    "\n",
    "    def save_logs_to_csv(self, path, dataset=None):\n",
    "        \"\"\"\n",
    "        save logs to a csv file. Provide the path, optionally provide a dataset name. Creates new column.\n",
    "        \"\"\"\n",
    "        if self.optimize == 'default':\n",
    "            cols = ['adjusted_rand_score', 'homogeneity_completeness_v_measure',\n",
    "           'homogeneity_score', 'v_measure_score', 'completeness_score',\n",
    "           'adjusted_mutual_info_score', 'relative_validity', 'probability',\n",
    "           'persistence', 'penalty', 'outlier', 'score', 'cluster_size', 'val_w',\n",
    "           'prob_w', 'pers_w', 'penalty_w', 'outlier_w',\n",
    "           'cluster_selection_epsilon', 'cluster_selection_method', 'metric',\n",
    "           'min_cluster_size', 'n_components', 'n_neighbors']\n",
    "        elif self.optimize == 'rand_score':\n",
    "            cols = ['relative_validity', 'probability', 'persistence', 'penalty', 'outlier', 'cluster_size',\n",
    "                    'adjusted_rand_score', 'homogeneity_completeness_v_measure', 'homogeneity_score', 'v_measure_score',\n",
    "                    'completeness_score', 'adjusted_mutual_info_score', 'score']\n",
    "        \n",
    "        df = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        df = df.append(self.logs)\n",
    "        \n",
    "        if dataset!=None:\n",
    "            df['dataset'] = dataset\n",
    "        #df.to_csv(path, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BayesClusterTrainer.generate_clusters\" class=\"doc_header\"><code>BayesClusterTrainer.generate_clusters</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BayesClusterTrainer.generate_clusters</code>(**`embeddings`**, **`min_cluster_size`**, **`cluster_selection_epsilon`**, **`cluster_selection_method`**, **`metric`**, **`n_neighbors`**, **`n_components`**, **`random_state`**=*`42`*)\n",
       "\n",
       "Generate HDBSCAN cluster object after reducing embedding dimensionality with UMAP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BayesClusterTrainer.generate_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BayesClusterTrainer.objective\" class=\"doc_header\"><code>BayesClusterTrainer.objective</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BayesClusterTrainer.objective</code>(**`params`**, **`embeddings`**, **`labels`**)\n",
       "\n",
       "Objective function for hyperopt to minimize, which incorporates constraints\n",
       "on the number of clusters we want to identify"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BayesClusterTrainer.objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BayesClusterTrainer.score_clusters\" class=\"doc_header\"><code>BayesClusterTrainer.score_clusters</code><a href=\"__main__.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BayesClusterTrainer.score_clusters</code>(**`clusters`**, **`y`**, **`params`**)\n",
       "\n",
       "Returns the label count and cost of a given cluster supplied from running hdbscan"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BayesClusterTrainer.score_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BayesClusterTrainer.train\" class=\"doc_header\"><code>BayesClusterTrainer.train</code><a href=\"__main__.py#L154\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BayesClusterTrainer.train</code>(**`max_evals`**=*`100`*, **`algo`**=*`suggest`*)\n",
       "\n",
       "Perform bayesian search on hyperopt hyperparameter space to minimize objective function"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BayesClusterTrainer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BayesClusterTrainer.save_logs_to_csv\" class=\"doc_header\"><code>BayesClusterTrainer.save_logs_to_csv</code><a href=\"__main__.py#L189\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BayesClusterTrainer.save_logs_to_csv</code>(**`path`**, **`dataset`**=*`None`*)\n",
       "\n",
       "save logs to a csv file. Provide the path, optionally provide a dataset name. Creates new column."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BayesClusterTrainer.save_logs_to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data\n",
    "\n",
    "label_list = [np.array([i]*250) for i in range(1,5)]\n",
    "labels = np.hstack(label_list)\n",
    "\n",
    "embedding_list = [np.random.randn(250, 768) + i for i in range(10,50,10)]\n",
    "embeddings = np.vstack(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000, 768))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape , embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_rand_score : 0.0                                                       \n",
      "homogeneity_completeness_v_measure : (1.0, 0.0, 0.0)                            \n",
      "homogeneity_score : 1.0                                                         \n",
      "v_measure_score : 0.0                                                           \n",
      "completeness_score : 0.0                                                        \n",
      "adjusted_mutual_info_score : 0.0                                                \n",
      "--------------------                                                            \n",
      "SCORE: -0.0                                                                     \n",
      "adjusted_rand_score : 0.0                                                       \n",
      "homogeneity_completeness_v_measure : (1.0, 0.0, 0.0)                            \n",
      "homogeneity_score : 1.0                                                         \n",
      "v_measure_score : 0.0                                                           \n",
      "completeness_score : 0.0                                                        \n",
      "adjusted_mutual_info_score : 0.0                                                \n",
      "--------------------                                                            \n",
      "SCORE: -0.0                                                                     \n",
      "adjusted_rand_score : 0.0                                                       \n",
      "homogeneity_completeness_v_measure : (1.0, 0.0, 0.0)                            \n",
      "homogeneity_score : 1.0                                                         \n",
      "v_measure_score : 0.0                                                           \n",
      "completeness_score : 0.0                                                        \n",
      "adjusted_mutual_info_score : 0.0                                                \n",
      "--------------------                                                            \n",
      "SCORE: -0.0                                                                     \n",
      "100%|█████████████████████████| 3/3 [00:14<00:00,  4.92s/trial, best loss: -0.0]\n",
      "best:\n",
      "{'cluster_selection_epsilon': 0.26464646464646463, 'cluster_selection_method': 'eom', 'metric': 'manhattan', 'min_cluster_size': 790, 'n_components': 5, 'n_neighbors': 41}\n",
      "--------------------\n",
      "--------------------\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# set parameters for optimization run\n",
    "\n",
    "space = {\n",
    "              'min_cluster_size':hp.choice('min_cluster_size', range(5, 1000)), \n",
    "              'cluster_selection_epsilon': hp.choice('cluster_selection_epsilon', np.linspace(0.05, 0.9, 100).tolist()),\n",
    "              'cluster_selection_method' : hp.choice('cluster_selection_method',['eom','leaf']),\n",
    "              'metric' : hp.choice('metric',['euclidean','manhattan']),\n",
    "              'n_neighbors': hp.choice('n_neighbors', range(2, 100)),\n",
    "              'n_components': hp.choice('n_components', range(2, 45))\n",
    "        }\n",
    "\n",
    "#optional\n",
    "cost_fn_param = {'val_w': 0.15, 'prob_w': 0.25, 'pers_w': 0.2 , 'penalty_w': 0.25, 'outlier_w': 0.15}\n",
    "\n",
    "\n",
    "\n",
    "trainer = BayesClusterTrainer(space, {}, embeddings, labels, optimize='rand_score')\n",
    "trainer.train(max_evals=3)\n",
    "\n",
    "#optional save logs to csv\n",
    "# df = trainer.save_logs_to_csv('test.csv', dataset='news_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(cost_fn_param) == dict\n",
    "assert type(space) == dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
